# 1. 普通最小二乘法原理

## 1.1  线性模型
线性模型是在实践中广泛使用的一类模型，几十年来被广泛研究，它可以追溯到一百多年前。线性模型利用输入特征的线性函数 （linear function）进行预测。

对于回归问题，线性模型预测的一般公式如下：
- $\hat{y} = w[0]*x[0]+w[0]*x[0]+...+w[p]*x[p]+b$

这里$x[0]$到$x[p]$表示单个数据点的特征(本例中特征个数为$p+1$)，$w$和$b$是学习模型的参数，$\hat{y}$是模型的预测结果。对于单一特征的数据集，公式如下：
- $\hat{y} = w[0]*x[0]+b$

这就是高中数学里的直线方程。这里$w[0]$是斜率，$b$是$y$轴偏移。对于有更多特征的数据集，$w$包含沿每个特征坐标轴的斜率。或者，你也可以将预测的响应值看作输入特征的加权求和，权重由$w$的元素给出(可以取负值)。

下图是《Python机器学习基础教程》的图2-11，显示了使用线性模型预测wave数据集的结果：
<img src="./image/01.png" style="zoom:80%">

用于回归的线性模型可以表示为这样的回归模型：对单一特征的预测结果是一条直线，两个特征时是一个平面，或者在更高维度时是一个超平面。

如果将直线的预测结果与kNN的预测结果进行比较，你会发现直线的预测能力非常受限。似乎数据的所有细节都丢失了。从某种意义上来说，这种说法是正确的。假设目标$y$是特征的线性组合，这是一个非常强的（也有点不现实的）假设。但观察一维数据得出的观点有些片面。对于有多个特征的数据集而言，线性模型可以非常强大。特别地，如果特征数量大于训练数据点的数量，任何目标$y$都可以（在训练集上）用线性函数完美拟合。

有许多不同的线性回归模型。这些模型之间的区别在于如何从训练数据中学习参数$w$和$b$，以及如何控制模型复杂度。



## 1.2 最小二乘法
普通最小二乘法（ordinary least squares，OLS），或者称为线性回归，是回归问题最简单也最经典的线性方法。最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：
- $目标函数 = \sum(观测值-理论值)^2$

观测值就是我们的多组样本，理论值就是我们的假设拟合函数。目标函数也就是在机器学习中常说的损失函数，我们的目标是得到使目标函数最小化时候的拟合函数的模型，即保证所有数据偏差的平方和最小（“平方”的在古时侯的称谓为“二乘”）。

最小二乘法的模型可以写为：
- $\hat{y}_{i} = w_{1}x_{i1} + w_{2}x_{i2} + ... + w_{k}x_{ik} + e_{i} ,   1\le i\le n,   k\ge 1$

其中，$x_i$表示输入的特征特征向量、$\hat{y}_i$表示根据模型预测的标签向量、$w_i$是每个维度特征的斜率、$e_i$是每个维度特征的截距。

用矩阵形式表示上面的公式，其中$\hat{y}$表示预测的值向量、$W$表示模型参数向量、$X$表示特征矩阵、$e$表示error：
- $\hat{y} = W^TX + e$
- $\left[ \begin{matrix} \hat{y}_{1}\\   \hat{y}_{2}\\   \vdots \\   \hat{y}_{n}\\ \end{matrix} \right] =  \left[ \begin{matrix} x_{11}&  x_{12}&  \cdots &  x_{1k}\\      x_{21}&  x_{22}&  \cdots &  x_{2k}\\      \vdots &  \vdots &  \ddots &  \vdots \\     x_{n1}&  x_{n2}&  \cdots &  x_{nk}\\ \end{matrix} \right] \left[ \begin{matrix} w_{1}\\   w_{2}\\   \vdots \\   w_{k}\\ \end{matrix} \right] +  \left[ \begin{matrix} e_{1}\\   e_{2}\\   \vdots \\   e_{n}\\ \end{matrix} \right]$

方便起见，我们用$\alpha$表示参数向量，即$\hat{y} = XW^T + e = X\alpha+e$。假设$y$是特征的真实标签，则预测值和真实值之间的偏差为$y-\hat{y} = y - X\alpha -e$，当向量$e$尽量小的时候，预测值和真实值越接近，即$e$的长度是我们优化的目标：
- $|e| = \sqrt { \sum ^{n}_{i=1}e_{i}^{2} }$

我们把求平方根运算转换为平方：
- $|e|^{2} = \sum ^{n}_{i=1}e_{i}^{2} = ee = e^{T}e$

即：当$e^{T}e$取得最小值时，$W$能取得最优解。



## 1.3 公式推导
由上文可知：
- $e = \hat{y} - X\alpha$
- $e^{T} = (\hat{y} - X\alpha)^{T}$
- $e^{T}e  = (\hat{y} - X\alpha)^{T}(\hat{y} - X\alpha)$
- $\   \     = (\hat{y}^{T} - \alpha^{T}X^{T})(\hat{y} - X\alpha)$
- $\   \     = \hat{y}^{T}\hat{y} - \alpha^{T}X^{T}\hat{y} - \hat{y}^{T}X\alpha + \alpha^{T}X^{T}X\alpha$

上式中重要的是中间的2个子项是可以合并的。首先，仔细观察$\alpha^{T}X^{T}\hat{y}$这个子项，发现它是一个值，一个数值可认为是一个1维的方阵，1维方阵的转置矩阵是它本身。那么就有：
- $\alpha^{T}X^{T}\hat{y}  = (\alpha^{T}X^{T}\hat{y})^{T} = \hat{y}^{T}(\alpha^{T}X^{T})^{T} = \hat{y}^{T}(X\alpha) = \hat{y}^{T}X\alpha$

所以上面的方程可变为：
- $e^{T}e = \hat{y}^{T}\hat{y} - 2\hat{y}^{T}X\alpha + \alpha^{T}X^{T}X\alpha$

如何让$e^{T}e$取得最小值？此时需要使用矩阵微分求解。



## 1.4 矩阵微分
### 1.4.1 矩阵微分公式
设：
- $\vec{y} = A \vec{x}$ 

其中，$y$是一个$m×1$的矩阵，$A$是一个$m×n$的矩阵，$x$是一个$n×1$的矩阵。则有：
- $\frac{\partial{\vec{y}}}{\partial{\vec{x}}} =A  \    \ 【公式1】$

这是如何得到的呢？实际上超级简单，上面这个式子指的是，$\vec{y}$的每一个分量对$\vec{x}$的每一个分量的微分，结果显然就是一个$m×n$矩阵。

### 4.2 扩展公式
设：
$F = \vec{y}^TA\vec{x}$

则有：
+ $\frac{\partial F }{\partial \vec{x}} = \vec{y}^{T}A   \   \  【公式2】$
+ $\frac{\partial F}{\partial \vec{y}} = \vec{x}^{T}A^{T}   \   \   【公式3】$

设：
$F = \vec{x}^{T}A\vec{x}$

且A是对称矩阵，则有：
+ $\frac{\partial F}{\partial \vec{x}} = 2\vec{x}^{T}A     \     \   【公式4】$

### 4.3 应用矩阵微分公式
利用矩阵微分方程求解下列等式：
+ $e^{T}e = \hat{y}^{T}\hat{y} - 2\hat{y}^{T}X\alpha + \alpha^{T}X^{T}X\alpha$

对等号右边的式子求关于$\alpha$的微分，得到：
+ $\frac{\partial \hat{y}^{T}\hat{y}}{\partial \alpha} - 2\frac{\partial \hat{y}^{T}X\alpha}{\partial \alpha} + \frac{\partial \alpha^{T}X^{T}X\alpha}{\partial \alpha}$

当这个式子(导数)等于$0$时, 就得到了$e^{T}e$的最小值。显然，第一个子项为0，所以可把它去掉，得到：
- $-2 \frac{\partial \hat{y}^TX\alpha}{\partial \alpha} + \frac{\partial \alpha^TX^TX\alpha}{\partial \alpha} = 0$
- $2 \frac{\partial \hat{y}^TX\alpha}{\partial \alpha} = \frac{\partial \alpha^TX^TX\alpha}{\partial \alpha}$

观察左边的式子，和上文的【公式2】是一样的，所以有：
- $2\frac{\partial \hat{y}^{T}X\alpha}{\partial \alpha} = 2\hat{y}^{T}X$

观察右边的式子，符合上文的【公式4】，所以有：
- $\frac{\partial \alpha^{T}X^{T}X\alpha}{\partial \alpha} = 2\alpha^{T}X^{T}X$

综上，得：
+ $2\hat{y}^{T}X = 2\alpha^{T}X^{T}X$
+ $\hat{y}^{T}X = \alpha^{T}X^{T}X$
+ $(\hat{y}^{T}X)^T =(\alpha^{T}X^{T}X)^T$
+ $X^{T}\hat{y} = X^{T}X\alpha$

最终得到：
+ $\alpha = (X^TX)^{-1}X^T\hat{y}$

这个公式就是所谓的最小二乘估计(Least Squares Estimator)了。






