# 逻辑回归之梯度下降法

## 1.  总述
`逻辑回归算法原理`部分介绍了逻辑回归算法的基本原理，本文使用梯度下降算法求解逻辑回归。

《Marchine Learning in Action》第五章介绍了逻辑回归(Logistic regression)，但是书中没有给出目标函数，也没有给出梯度下降法的推导。在解释代码处文中指出：
> A little math is needed to derive the equations used here, and I’ll leave you to look into that further if desired.

本文的目的就是展示所谓的`A little math`。



## 2. 原理
逻辑回归使用$Sigmoid$函数或逻辑函数进行分类，$Sigmoid$函数定义如下：
- $f(X) = \frac{1}{1+e^{-\theta^TX}}$

其中，$\theta$是参数向量，$X$是自变量（向量）。

逻辑分类使用对数损失函数。我们用$L$来表示损失函数（取$Loss$之意），则对数损失函数的表达式为：
- $L(\theta ) = - \frac{1}{N}\sum\limits_{i = 1}^n {\left[ {{y_i}\log \left( {\frac{1}{{1 + {e^{ - {\theta ^T}{X_i}}}}}} \right) + (1 - {y_i})\log \left( {1 - \frac{1}{{1 + {e^{ - {\theta ^T}{X_i}}}}}} \right)} \right]}$

因此，我们只要找到一个参数向量$\theta$，能使得此式的值最小，那么这个参数向量$\theta$就是“最优”的参数向量。

求得了这个最优的$\theta$之后，把它代入$Sigmoid$公式，则对任一个未知的$X$，我们都可以计算出$f(X)$值，然后再根据一个阈值把它调整到0或1，就得到了这个$X$所属的分类，这样，我们就完成了一次“预测”的过程。



## 3. 一点点数学
后续的推导会用到下面列举的导数公式：
1. $y = C, (C是任意的常数)， 则 y'=0$
2. $y = e^x, y' = e^x$
3. $y = e^{-x}, y'= -e^{-x}$
4. $y = log(u), y' = \frac{1}{u}u'$
5. $f = (y^x), f' = xy^{x-1}x'$
6. 函数的和、差、积、商的求导法则：$u = u(x), v = v(x)$, $u$和$v$都可导, 则：
	6.1 $(u\pm v) = u'\pm v'$
	6.2 $(uv)' = u'v + uv'$
	6.3 $(Cu) = Cu'$
	6.4 $(\frac{u}{v})' = \frac{u'v-uv'}{v^2}$
7. 复合函数求导：设$y = f(u)，u = \phi(x)$，且$f(u)、u = \phi(x)$都可导，则复合函数$y=f[\phi(x)]$的导数为$y' = f'(u)*\phi'(x)$



## 4. Loss function的推导过程
我们已经知道，逻辑回归的Loss function的为：
- $L(\theta ) = - \frac{1}{N}\sum_{i = 1}^n {\left[ {y_i\log \left( {\frac{1}{1+e^{-\theta^TX_i}}} \right) + (1-y_i)\log \left( {1 - \frac{1}{1+e^{-\theta ^TX_i}}} \right)} \right]}$

求$L(\theta)$对$\theta$的偏导数：
1. 根据求导公式6.3，可以先把常数项$-\frac{1}{N}\sum_{i=1}^N$提取出来，即$L(\theta)'' = -\frac{1}{N}\sum_{i=1}^NK(\theta)'$

2. 为了方便显示，我们先去掉表示样本个数的脚标$i$，并把$Sigmoid$函数用$h_\theta$表示，则$K(\theta)'$可以表示为：$K(\theta)' = ( y \cdot log(h_\theta(x)) + (1-y) \cdot log(1-h_\theta(x)) )'$

3. 根据对数复合求导公式4，$K(\theta)' = y \cdot \frac{1}{h_\theta(x)}h_\theta(x)' + (1-y)\frac{1}{1-h_\theta(x)}(1-h_\theta(x))'$。现在的问题转换为对$h_\theta(x)$和$(1-h_\theta(x))$求导

4. $h_\theta(x)' = (\frac{1}{1+e^{-\theta^TX}})'$












