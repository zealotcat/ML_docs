# 逻辑回归之梯度下降法

## 1.  总述
《逻辑回归算法原理》一文介绍了逻辑回归算法的基本原理，本文使用梯度下降算法求解逻辑回归。

《Marchine Learning in Action》第五章介绍了逻辑回归(Logistic regression)，但是书中没有给出目标函数，也没有给出梯度下降法的推导。在解释代码处文中指出：
> A little math is needed to derive the equations used here, and I’ll leave you to look into that further if desired.

本文的目的就是展示所谓的`A little math`。



## 2. 原理
逻辑回归使用$Sigmoid$函数或逻辑函数进行分类，$Sigmoid$函数定义如下：
- $f(X) = \frac{1}{1+e^{-\theta^TX}}$

其中，$\theta$是参数向量，$X$是自变量（向量）。

逻辑分类使用对数损失函数。我们用$L$来表示损失函数（取$Loss$之意），则对数损失函数的表达式为：
- $L(\theta ) = - \frac{1}{N}\sum\limits_{i = 1}^n {\left[ {{y_i}\log \left( {\frac{1}{{1 + {e^{ - {\theta ^T}{X_i}}}}}} \right) + (1 - {y_i})\log \left( {1 - \frac{1}{{1 + {e^{ - {\theta ^T}{X_i}}}}}} \right)} \right]}$

因此，我们只要找到一个参数向量$\theta$，能使得此式的值最小，那么这个参数向量$\theta$就是“最优”的参数向量。

求得了这个最优的$\theta$之后，把它代入$Sigmoid$公式，则对任一个未知的$X$，我们都可以计算出$f(X)$值，然后再根据一个阈值把它调整到0或1，就得到了这个$X$所属的分类，这样，我们就完成了一次“预测”的过程。

## 3. 一点点数学
























