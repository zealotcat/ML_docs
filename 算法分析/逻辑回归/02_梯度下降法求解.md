# 2. 梯度下降法求解

## 2.1 概述
`逻辑回归算法原理`部分介绍了逻辑回归算法的基本原理，本文使用梯度下降算法求解逻辑回归。

《Marchine Learning in Action》第五章介绍了逻辑回归(Logistic regression)，但是书中没有给出目标函数，也没有给出梯度下降法的推导。在解释代码处文中指出：
> A little math is needed to derive the equations used here, and I’ll leave you to look into that further if desired.

本文的目的就是展示所谓的`A little math`。



## 2.2 原理
逻辑回归使用$Sigmoid$函数或逻辑函数进行分类，$Sigmoid$函数定义如下：
- $f(X) = \frac{1}{1+e^{-\theta^TX}}$

其中，$\theta$是参数向量，$X$是自变量（向量）。

逻辑分类使用对数损失函数。我们用$L$来表示损失函数（取$Loss$之意），则对数损失函数的表达式为：
- $L(\theta ) = - \frac{1}{N}\sum_{i=1}^N {\left[ {{y^{(i)}}\log \left( {\frac{1}{{1 + {e^{ - {\theta ^T}{X}}}}}} \right) + (1 - {y^{(i)}})\log \left( {1 - \frac{1}{{1 + {e^{ - \theta ^TX}}}}} \right)} \right]}$

因此，我们只要找到一个参数向量$\theta​$，能使得此式的值最小，那么这个参数向量$\theta​$就是“最优”的参数向量。

求得了这个最优的$\theta$之后，把它代入$Sigmoid$公式，则对任一个未知的$X$，我们都可以计算出$f(X)$值，然后再根据一个阈值把它调整到0或1，就得到了这个$X$所属的分类，这样，我们就完成了一次“预测”的过程。



## 2.3 一点点数学
后续的推导会用到下面列举的导数公式：
1. $y = C, (C是任意的常数)， 则 y'=0$
2. $y = e^x, y' = e^x$
3. $y = e^{-x}, y'= -e^{-x}$
4. $y = log(u), y' = \frac{1}{u}u'$
5. $f = (y^x), f' = xy^{x-1}x'$
6. 函数的和、差、积、商的求导法则：$u = u(x), v = v(x)$, $u$和$v$都可导, 则：
	6.1 $(u\pm v) = u'\pm v'$
	6.2 $(uv)' = u'v + uv'$
	6.3 $(Cu) = Cu'$
	6.4 $(\frac{u}{v})' = \frac{u'v-uv'}{v^2}$
7. 复合函数求导：设$y = f(u)，u = \phi(x)$，且$f(u)、u = \phi(x)$都可导，则复合函数$y=f[\phi(x)]$的导数为$y' = f'(u)*\phi'(x)$



## 2.4 Loss function偏导数推导
我们已经知道，逻辑回归的Loss function的为：
- $L(\theta ) = - \frac{1}{N}\sum_{i=1}^N {\left[ {{y^{(i)}}\log \left( {\frac{1}{{1 + {e^{ - {\theta ^T}{X}}}}}} \right) + (1 - {y^{(i)}})\log \left( {1 - \frac{1}{{1 + {e^{ - \theta ^TX}}}}} \right)} \right]}$

求$L(\theta)$对$\theta$的偏导数：
1. 根据求导公式6.3，可以先把常数项$-\frac{1}{N}\sum_{i=1}^N$提取出来，即：
$L(\theta)' = -\frac{1}{N}\sum_{i=1}^{N}K(\theta)'$


2. 为了方便显示，我们先去掉表示样本个数的角标$i$，并把$Sigmoid$函数用$h_\theta$表示，则$K(\theta)'$可以表示为：
$K(\theta)' = ( y \cdot log(h_\theta(x)) + (1-y) \cdot log(1-h_\theta(x)) )'$


3. 根据对数复合求导公式4，我们可以将上面的等式转换为：
$K(\theta)' = y \cdot \frac{1}{h_\theta(x)}h_\theta(x)' + (1-y)\frac{1}{1-h_\theta(x)}(1-h_\theta(x))'$，现在的问题转换为对$h_\theta(x)$和$(1-h_\theta(x))$求导


4. 根据常数求导公式1和商求导公式6.4，推导出：
$h_\theta(x)' = (\frac{1}{1+e^{-\theta^TX}})' = \frac{1' \cdot (e^{-\theta^TX}) - 1 \cdot (e^{-\theta^TX})'}{(1+e^{-\theta^TX})^2} = \frac{-(e^{-\theta^TX})'}{(1+e^{-\theta^TX})^2}$


5. 再利用复合函数求导公式，上式可以进行推导为：$h_\theta(x)' = \frac{-(-e^{-\theta^TX}) \cdot (\theta^TX)'}{(1+e^{-\theta^TX})^2} = \frac{e^{-\theta^TX}}{(1+e^{-\theta^TX})^2} \cdot (\theta^TX)'​$


6. 继续分解：
$h_\theta(x)' = \frac{1}{1+e^{-\theta^TX}} \cdot \frac{e^{-\theta^TX}}{1+e^{-\theta^TX}} \cdot (\theta^TX)' $
$  \   \   = \frac{1}{1+e^{-\theta^TX}} \cdot \frac{e^{-\theta^TX}+1-1}{1+e^{-\theta^TX}} \cdot (\theta^TX)' $
$ \   \  = \frac{1}{1+e^{-\theta^TX}} \cdot \frac{(1+e^{-\theta^TX})-1}{1+e^{-\theta^TX}} \cdot (\theta^TX)'$
$ \   \  = \frac{1}{1+e^{-\theta^TX}} \cdot (1-\frac{1}{1+e^{-\theta^TX}}) \cdot (\theta^TX)'$
$ \   \  = h_\theta(x) \cdot (1-h_\theta(x)) \cdot (\theta^TX)'$


7. 根据上一步骤的最后结果，我们可以得到：
$(1-h_\theta(x))'  = -h_\theta(x) \cdot (1-h_\theta(x)) \cdot (\theta^TX)'$


8. 至此，我们得到两个等式。将这两个式子带入到步骤3得出的等式可以得出：
$K(\theta)' = y \cdot \frac{1}{h_\theta(x)}h_\theta(x)' + (1-y)\frac{1}{1-h_\theta(x)}(1-h_\theta(x))'$
$\   \   = y \cdot \frac{1}{h_\theta(x)} \cdot h_\theta(x) \cdot (1-h_\theta(x)) \cdot (\theta^TX)' + (1-y) \cdot \frac{1}{1-h_\theta(x)} \cdot (-h_\theta(x)) \cdot (1-h_\theta(x)) \cdot (\theta^TX)'$


9. 消除相等的因子之后，得到$K(\theta)'$的最终等式：
$K(\theta)' = y \cdot (1-h_\theta(x)) \cdot (\theta^TX)' + (1-y) \cdot (-h_\theta(x)) \cdot (\theta^TX)' = (y - h_\theta(x))(\theta_TX)'$


10. 将$K(\theta)'$的等式带回到步骤1的等式得到：
$L(\theta)' = -\frac{1}{N}\sum_{i=1}^{N}(y-h_\theta(x))(\theta^TX)'$


11. 由于$\theta^TX = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$，那么$\theta^TX$对第$j$个参数$\theta$求偏导数结果为$\frac{\partial \theta^TX}{\partial \theta_j} = x_j$，所以$L(\theta)'$对$\theta_j$的偏导数为：
$\frac{\partial L(\theta)'}{\partial \theta_j} = \frac{1}{N}\left[\sum_{i=1}^{N}(h_\theta(x^{(i)})-y^{(i)})x_j^i\right]$




## 2.5 具体实现
参考《机器学习实战》代码：
```python
def sigmoid(inX):
    return 1.0/(1+np.exp(-inX))

# 梯度上升
def gradAscent(dataMatIn, classLabels):
    # 转换为NumPy矩阵数据类型 
    dataMatrix = np.mat(dataMatIn)
    labelMat = np.mat(classLabels).transpose()
    m,n = np.shape(dataMatrix)
    alpha = 0.001
    maxCycles = 500
    weights = np.ones((n,1))
    for k in range(maxCycles):
        # 矩阵相乘
        h = sigmoid(dataMatrix*weights)
        error = (labelMat - h)
        weights = weights + alpha * dataMatrix.transpose()* error
    return weights
```

具体的使用方法参考《机器学习实战》关于逻辑回归的章节。