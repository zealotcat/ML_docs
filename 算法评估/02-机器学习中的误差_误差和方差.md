## 机器学习中的Bias(偏差)、Error(误差)和Variance(方差)
https://www.zhihu.com/question/27068705

机器学习(ML)、自然语言处理(NLP)、信息检索(IR)等领域，评估(Evaluation)是一个必要的工作，而其评价指标往往有如下几个参数：准确率(Accuracy)、正确率(Precision)、召回率(Recall)和F1-Measure，以及相关的几条曲线：PR曲线、ROC曲线。

### 1. 正确率和召回率
先假定一个具体场景作为例子：
> 假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生. 现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了. 作为评估者的你需要来评估(evaluation)下他的工作

首先我们可以计算**准确率(accuracy)**,其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是0-1损失时测试数据集上的准确率。

这样说听起来有点抽象，简单说就是，前面的场景中，实际情况是那个班级有男的和女的两类，某人（也就是定义中所说的分类器）通过某种算法把班级中的人分为男女两类。accuracy需要得到的是此人分类正确的人占总人数的比例。很容易，我们可以得到：他把其中70（20女+50男）人判定正确了，而总人数是100人，所以它的**准确率(accuracy)**就是70%。

**准确率(accuracy)**确实是一个很好很直观的评价指标，但是有时候**准确率(accuracy)**高并不能代表一个算法就好。比如某个地区某天地震的预测，假设我们有一堆的特征作为地震分类的属性，类别只有两个：0-不发生地震、1-发生地震，其中，类别1的特征数量远远小于类别0。假设有一个不加思考的分类器，即使对每一个测试用例都将类别划分为0，那么它也有可能达到99%的正确率，但真的地震来临时，这个分类器毫无察觉，这个分类带来的损失是巨大的。为什么99%的**准确率(accuracy)**的分类器却不是我们想要的？因为这里数据分布不均衡，类别1的数据太少，完全错分类别1依然可以达到很高的正确率却忽视了我们关注的东西。

因此，我们使用**正确率(Precision)**、**召回率(Recall)**和**F1-Measure**来评估一个分类器.

在说**正确率(Precision)**、**召回率(Recall)**和**F1-Measure**之前，我们先定义TP、FN、FP、TN四种分类情况。

在二分类问题中，分类器将一个实例的分类标记为是或否，这可以用一个混淆矩阵来表示。混淆矩阵有四个分类，如下表：
![混淆矩阵](./image/01.png)

其中，列对应于实例实际所属的类别，行表示分类预测的类别。以下是常用术语定义：
- TP（True Positive）：正确分类的正样本数
- FP（False Positive）：被错误的标记为正样本的负样本数，实际为负样本
- TN（True Negative）：正确分类的负样本数
- FN（False Negative）：指被错误的标记为负样本的正样本数，实际为正样本
- TP+FP+TN+FN：样本总数
- TP+FN：实际正样本数。
- TP+FP：预测结果为正样本的总数，包括预测正确的和错误的
- FP+TN：实际负样本数
- TN+FN：预测结果为负样本的总数，包括预测正确的和错误的

这里面的概念有些绕，需要慢慢理解。以这四个基本指标可以衍生出多个分类器评价指标，如下图：
- $precision = \frac{TP}{TP+FP}$
- $recall = \frac{TP}{TP+FN}$
- $tpr(true positive rate) = recall =  \frac{TP}{TP+FN}$
- $fpr(false positive rate) = \frac{FP}{FP+TN}$

其中，TPR定义为`真正样本率`，指实际正样本中被预测正确的概率；FPR为`假正样本率`，指实际负样本中被错误预测为正样本的概率。

### 2. PR曲线
**正确率(Precision)**和**召回率(Recall)**是一对矛盾的个体，一般来说，**正确率(Precision)**高则**召回率(Recall)**会低，、**召回率(Recall)**高则**正确率(Precision)**会低：
![正确率和召回率](./image/02.png)

为了让我们综合地来考量一个模型的好坏，就有了PR曲线：
![PR](./image/03.png)

这里横轴是**召回率(Recall)**，纵轴是**正确率(Precision)**，而PR曲线就是一个向右上方凸出的曲线，越接近点(1,1)，算法效果越好。

### 3. ROC曲线
ROC（Receiver Operating Characteristic）曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣。下图是一个ROC曲线的示例：
![ROC](./image/04.png)

正如我们在这个ROC曲线的示例图中看到的那样，ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）。

接下来我们考虑ROC曲线图中的四个点和一条线：
1. 点(0,1)，即FPR=0, TPR=1，这意味着FN(false negative) = 0，并且FP(false positive) = 0。Wow，这是一个完美的分类器，它将所有的样本都正确分类
2. 点(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案
3. 点(0,0)，即FPR=TPR=0，即FP(false positive) = TP(true positive) =0，可以发现该分类器预测所有的样本都为负样本（negative）
4. 点(1,1)，分类器实际上预测所有的样本都为正样本

经过以上的分析，我们可以断言，ROC曲线越接近左上角，该分类器的性能越好。

下面考虑ROC曲线图中的虚线y=x上的点。这条对角线上的点其实表示的是一个采用随机猜测策略的分类器的结果，例如(0.5,0.5)，表示该分类器随机对于一半的样本猜测其为正样本，另外一半的样本为负样本。

### 4. PR曲线和ROC曲线的绘制
绘制ROC曲线和PR曲线都是选定不同阈值，从而得到不同的x轴和y轴的值，画出曲线。例如，一个分类算法，找出最优的分类效果，对应到ROC空间中的一个点。通常分类器输出的都是score，如SVM、神经网络等，有如下预测效果：
![scores](./image/05.png)

True表示实际样本属性，Hyp表示预测结果样本属性，第4列即是Score，Hyp的结果通常是设定一个阈值，比如上表Hyp(0.5)和Hyp(0.6)就是阈值为0.5和0.6时的结果，Score>阈值为正样本，小于阈值为负样本，这样只能算出一个ROC值。

当阈值为0.5时，$TPR=\frac{6}{6+0} = 1$，$FPR=\frac{FP}{FP+TN}=\frac{2}{2+2}=0.5$,得到ROC的一个坐标为（0.5,1）；Recall=TPR=1，Precision=6/(6+2)=0.75，得到一个PR曲线坐标(1,0.75)。同理得到不同阈下的坐标，即可绘制出曲线。
![scores](./image/06.png)

### 5. ROC曲线和PR曲线的关系
在ROC空间，ROC曲线越凸向左上方向效果越好；与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。

ROC和PR曲线都被用于评估机器学习算法对一个给定数据集的分类性能，每个数据集都包含固定数目的正样本和负样本。而ROC曲线和PR曲线之间有着很深的关系。

> 定理1：对于一个给定的包含正负样本的数据集，ROC空间和PR空间存在一一对应的关系，也就是说，如果recall不等于0，二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。

> 定理2：对于一个给定数目的正负样本数据集，一条曲线在ROC空间中比另一条曲线有优势，当且仅当第一条曲线在PR空间中也比第二条曲线有优势。（这里的“一条曲线比其他曲线有优势”是指其他曲线的所有部分与这条曲线重合或在这条曲线之下。）

当正负样本差距不大的情况下，ROC和PR的趋势是差不多的，但是当负样本很多的时候，两者就截然不同了，ROC效果依然看似很好，但是PR上反映效果一般。解释起来也简单，假设就1个正例，100个负例，那么基本上TPR可能一直维持在1左右，然后突然降到0。如图，(a)(b)分别为正负样本1:1时的ROC曲线和PR曲线，二者比较接近。而(c)(d)的正负样本比例为1:100，这时ROC曲线效果依然很好，但是PR曲线则表现的比较差。这就说明ROC曲线在正负样本比例悬殊较大时更能反映分类的性能。
![pr vs. roc](./image/07.jpg)

### 6. AUC
AUC(Area Under Curve)即指曲线下面积占总方格的比例。有时不同分类算法的ROC曲线存在交叉，因此很多时候用AUC值作为算法好坏的评判标准。面积越大，表示分类性能越好。









