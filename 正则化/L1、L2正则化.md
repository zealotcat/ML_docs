# $L1、L2$正则化
所有的机器学习监督学习算法都可以归结为一句话：“minimize your error while regularizing your parameters”，即在规则化参数的同时最小化误差。最小化误差是为了让我们的模型尽可能拟合我们的训练数据，而规则化参数则是防止我们的模型过分拟合我们的训练数据。

从算法的角度看，如果参数太多，会导致我们的模型复杂度上升，也就是我们的训练误差会很小；但训练误差小并不是我们的最终目标，训练误差下容易导致过拟合。我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。换句话说，规则化符合奥卡姆剃刀(Occam's razor)原理：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。

另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。

## 1. 从经验风险最小化到结构经验最小化
机器学习里的损失函数（代价函数）可以用来描述模型与真模型（ground truth）之间的差距，因此可以解决“偏差”的问题。但是仅有损失函数，我们无法解决方差的问题，因而会有过拟合风险。$L1$正则项和$L2$正则项正好是损失函数的反面，通过引入正则项使机器学习模型避免过拟合。

经验风险最小化（empirical risk minimization，ERM）认为经验风险最小的模型是最优的模型，即求解最优化问题：

$\underset{f\in\mathscr{F}}{\min}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}$

其中，$y_i$是真实值，$f(x_i)$是根据模型算出来预测值，$L()$是损失函数，$N$是特征值的个数。

当样本容量足够大的时候，经验风险最小化学习效果良好。比如极大似然估计，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

但是当样本容量很小时，经验风险最小化学习会产生过拟合（over-fitting）的现象。这就引出了结构风险最小化，它等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），它的定义为：

$R_{srm}\left(f\right)=\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)$

其中$J(f)$为模型的复杂度，模型$f$越复杂，复杂度$J(f)$就越大；反之，模型越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚。$λ≥0$是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

结构风险最小化的策略认为结构风险最小的模型是最优的模型，求解最优模型即求解最优化问题：

$\min_{f\in\mathscr{F}}\frac{1}{N}\sum_{i=1}^N{L\left(y_i,f\left(x_i\right)\right)}+\lambda J\left(f\right)$

这样，监督学习问题变成了经验风险或结构风险函数的最优化问题。上式中，第一项是经验风险，第二项是正则化项，$λ≥0$为调整两者之间关系的系数。

## 2. 范数与正则项
在线性代数、函数分析等数学分支中，范数（Norm）是一个函数，其赋予某个向量空间（或矩阵）中的每个向量以长度或大小。

在实数域中，数的大小和两个数之间的距离是通过绝对值来度量的。在解析几何中，向量的大小和两个向量之差的大小是“长度”和“距离”的概念来度量的。为了对矩阵运算进行数值分析，我们需要对向量和矩阵的“大小”引进某种度量，即范数。所以，范数是具有“长度”概念的函数。范数是绝对值概念的自然推广。

范数的定义如下：
如果向量$x \in R^n$的某个实值函数$f(x) = ||x||$满足：
1. 正定性：$||x||\ge 0$，且$||x||=0$当且仅当x=0
2. 齐次性：对任意实数$c$，都有$||c x||=|c| ||x||$
3. 三角不等式：对任意$x,y \in R^n$，都有$||x+y||\le||x||+||y||$

则称$||x||$为$R^n$上的一个向量范数。

在这里，我们需要关注的最主要是范数的`非负性`。我们刚才讲，损失函数通常是一个有下确界的函数。而这个性质保证了我们可以对损失函数做最优化求解。如果我们要保证目标函数依然可以做最优化求解，那么我们就必须让正则项也有一个下界。非负性无疑提供了这样的下界，而且它是一个下确界——由齐次性保证（当$c=0$时）。

因此，我们说，范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是机器学习的学习目标——参数向量。

范数的一般化定义：设$p \ge 1$的实数，$p−norm$定义为：

$||x||_p:=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}  \ \ \  \ （1）$

机器学习中有几个常用的范数，分别是：
+ $L_1$范数：$||\vec{x}||=∑^d_{i=1}| x_i|$
+ $L_2$范数：$||\vec{x}||_2=(∑^d_{i=1}x^2_i)^{1/2}$
+ $L_p$范数：$||\vec{x}||_p=(\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}$
+ $L_\infty$范数：$||\vec{x}||_\infty=lim_{p→+∞}(∑^d_{i=1}x^p_i)^{1/p}$












