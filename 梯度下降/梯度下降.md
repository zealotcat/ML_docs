# 梯度下降(Gradient Descent)
在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。

https://www.cnblogs.com/pinard/p/5970503.html

## 1. 梯度
在微积分里面，对多元函数的参数求$\partial$偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数$f(x,y)$，分别对$x,y$求偏导数，求得的梯度向量就是$(\partial f/\partial x, \partial f/\partial y)^T$，简称$grad f(x,y)$或者$\bigtriangledown f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$(\partial f/\partial x_0, \partial f/\partial y_0)^T$，或者$\bigtriangledown f(x_0,y_0)$；如果是3个参数的向量梯度，就是$(\partial f/\partial x, \partial f/\partial y，\partial f/\partial z)^T$，以此类推。

那么这个梯度向量求出来有什么意义呢？它的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数$f(x,y)$，在点$(x_0,y_0)$，沿着梯度向量的方向（即$(\partial f/\partial x_0, \partial f/\partial y_0)^T$的方向）是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$-(\partial f/\partial x_0, \partial f/\partial y_0)^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。

## 2. 梯度下降与梯度上升
在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数$f(θ)$的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数$-f(θ)$的最大值，这时梯度上升法就派上用场了。

下面来详细总结下梯度下降法。

## 3. 梯度下降法算法详解
### 3.1 梯度下降的直观解释
首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

![gd01](./image/gd01.png)

### 3.2 梯度下降的相关概念
在详细了解梯度下降的算法之前，我们先看看相关的一些概念：
1. 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度
2. 特征（feature）：指的是样本中输入部分，比如样本$(x_0, y_0)$、$(x_1, y_1)$，则样本特征为$x$，样本输出为$y$
3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_\theta(x)$。比如对于样本$(x_i, y_i)(i=1,2,...n)$，可以采用拟合函数如下：$h_\theta(x) = θ_0+θ_1x$
4. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。

在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本$(x_i, y_i)(i=1,2,...n)$，采用线性回归，损失函数为：
$J(\theta_0, \theta_1) = \sum\limits_{i=1}^{m}(h_\theta(x_i) - y_i)^2$

其中$x_i$表示样本特征$x$的第$i$个元素，$y_i$表示样本输出$y$的第$i$个元素，$h_\theta(x_i)$为假设函数，损失函数为残差的平方和。而我们的目的就是最小化假设函数值，也就是最小化残差的平方和（residual sum of squares，RSS）。

有时，我们也会采用均方差(MSE)来衡量损失，即：
$J(\theta_0, \theta_1) =\frac{1}{2m} \sum\limits_{i=1}^{m}(h_\theta(x_i) - y_i)^2$

### 3.3 梯度下降的详细算法
梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。

### 3.4 梯度下降法的代数法
#### 3.4.1 算法描述
1. 先决条件： 确认优化模型的假设函数和损失函数。
   比如对于线性回归，假设函数表示为 $h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}$，其中，$θ_i (i = 0,1,2... n)$为模型参数，$x_i (i = 0,1,2... n)$为每个样本的n个特征值。这个表示可以简化，我们增加一个特征$x_0=1$ ，这样: 
   $h_\theta(x_0, x_1, ...x_n) = \sum\limits_{i=0}^{n}\theta_{i}x_{i}$。
   同样是线性回归，对应于上面的假设函数，损失函数为：
   $J(\theta_0, \theta_1..., \theta_n) = \frac{1}{2m}\sum\limits_{i=0}^{m}(h_\theta(x_0, x_1, ...x_n) - y_i)^2$

2. 算法相关参数初始化：主要是初始化$\theta_0$、$\theta_1$、...、$\theta_n$，算法终止距离$\varepsilon$以及步长$\alpha$。在没有任何先验知识的时候，可以将所有的$\theta$初始化为0、 将步长$\alpha$初始化为1。在调优的时候再优化。

3. 算法过程：
    1. 确定当前位置的损失函数的梯度，对于$\theta_i$，其梯度表达式如下：$\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1..., \theta_n)$
    2. 用步长乘以损失函数的梯度，得到当前位置下降的距离，即$\alpha\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1..., \theta_n)$对应于前面登山例子中的某一步
    3. 确定是否所有的θ~i~梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的θ~i~(i=0,1,...n)即为最终结果。否则进入步骤4
    4. 更新所有的θ，对于θ~i~，其更新表达式如下：$\theta_i = \theta_i - \alpha\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1..., \theta_n)$。更新完毕后继续转入步骤1

#### 3.4.2 基础算法示例
下面用Python实现一个代数法的梯度下降算法。我们使用向量平方和函数作为假设函数：
```python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
import math

# 向量平方和函数
def sum_of_squares(v):
    return sum(v_1 * v_2 for v_1, v_2 in zip(v, v))

# 向量平方和梯度函数(即平方函数的求导函数)
def sum_of_squares_gradient(v):
    return [2 * v_i for v_i in v]

# 步长函数
def step(v, direction, step_size):
    return [v_i + step_size * direction_i for v_i, direction_i in zip(v, direction)]

### 计算下降的距离，即损失函数
def magnitude(v):
    return math.sqrt(sum_of_squares(v))

def distance(v, w):
    return magnitude([v_i - w_i for v_i, w_i in zip(v,w)])

if __name__ == "__main__":
    print("using the gradient")
    v = [3,4,5,6,7,8,9]
    tolerance = 0.0000001

    while True:
        gradient = sum_of_squares_gradient(v)   # compute the gradient at v
        next_v = step(v, gradient, -0.01)       # take a negative gradient step
        if distance(next_v, v) < tolerance:     # stop if we're converging
            break
        v = next_v                              # continue if we're not

    print("minimum v", v)
    print("minimum value", sum_of_squares(v))
    print()
```

对于平方和函数，当输入0向量时，取值最小。测试结果如下：
```shell
alexmacbook:gradient_descent alex$ py gd.py
using the gradient
minimum v [8.898597636515473e-07, 1.186479684868729e-06, 1.4830996060859136e-06, 1.7797195273030945e-06, 2.076339448520277e-06, 2.372959369737458e-06, 2.6695792909546437e-06]
minimum value 2.463534574560851e-11
```

#### 3.4.3 线性回归例子
下面用线性回归的例子来具体描述梯度下降。假设我们的样本是：
$(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_n)$，

损失函数如前面先决条件所述：
$J(\theta_0, \theta_1..., \theta_n) = \frac{1}{2m}\sum\limits_{i=0}^{m}(h_\theta(x_0, x_1, ...x_n) - y_i)^2$。

那么，在算法过程步骤1中对于θ~i~的偏导数计算如下： 
$\frac{\partial}{\partial\theta_i}J(\theta_0, \theta_1..., \theta_n)= \frac{1}{m}\sum\limits_{j=0}^{m}(h_\theta(x_0^{j}, x_1^{j}, ...x_n^{j}) - y_j)x_i^{j}$。

由于样本中没有x~0~令上式中所有的x~j~^0^为1，步骤4中θ~i~的更新表达式如下：
$\theta_i = \theta_i - \alpha\frac{1}{m}\sum\limits_{j=0}^{m}(h_\theta(x_0^{j}, x_1^{j}, ...x_n^{j}) - y_j)x_i^{j}$

从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加1/m是为了好理解。由于步长也为常数，他们的乘机也为常数，所以这里$\alpha\frac{1}{m}$可以用一个常数表示。

在下面章节会详细讲到的梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是用所有样本。

